#!/usr/bin/env python3
"""
SPR{K}3 Vulnerability Scanner v4.4
Comprehensive security vulnerability detection for ML/AI codebases
Purpose: Defensive security tool to help developers find and fix vulnerabilities
License: Responsible use only - for security research and improvement
"""

import os
import sys
import re
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

class SPRk3VulnerabilityScanner:
    """
    Detects security vulnerabilities in ML/AI code to help developers
    write safer code and protect against attacks.
    """
    
    def __init__(self):
        self.vulnerabilities = []
        self.scan_count = 0
        
        # Comprehensive detection patterns for ML/AI vulnerabilities
        self.patterns = {
            # ========== DESERIALIZATION VULNERABILITIES ==========
            'torch_unsafe': {
                'pattern': r'torch\.load\([^)]+\)',
                'check': self._check_torch_load,
                'severity': 'CRITICAL',
                'cwe': 'CWE-502',
                'description': 'Unsafe torch.load() - potential arbitrary code execution',
                'recommendation': 'Add weights_only=True parameter'
            },
            'pickle_unsafe': {
                'pattern': r'pickle\.load',
                'severity': 'CRITICAL',
                'cwe': 'CWE-502',
                'description': 'Unsafe pickle deserialization - arbitrary code execution risk',
                'recommendation': 'Use safer serialization formats (JSON, MessagePack)'
            },
            'dill_load': {
                'pattern': r'dill\.load',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'dill.load() can execute arbitrary code',
                'recommendation': 'Use safer serialization methods'
            },
            'joblib_load': {
                'pattern': r'joblib\.load',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'joblib.load() may execute arbitrary code',
                'recommendation': 'Validate source and consider alternatives'
            },
            'sklearn_joblib': {
                'pattern': r'joblib\.load.*\.pkl',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Loading sklearn models with joblib can execute code',
                'recommendation': 'Use trusted sources only, consider ONNX export'
            },
            'numpy_load': {
                'pattern': r'numpy\.load\([^)]*allow_pickle\s*=\s*True',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'NumPy load with allow_pickle=True can execute code',
                'recommendation': 'Set allow_pickle=False'
            },
            'pandas_read_pickle': {
                'pattern': r'pd\.read_pickle|pandas\.read_pickle',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Pandas read_pickle can execute arbitrary code',
                'recommendation': 'Use safer formats like parquet or feather'
            },
            'yaml_unsafe': {
                'pattern': r'yaml\.load.*Loader\s*=\s*yaml\.(Loader|FullLoader|UnsafeLoader)',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Unsafe YAML loading - potential code execution',
                'recommendation': 'Use yaml.safe_load() or SafeLoader'
            },
            
            # ========== MODEL LOADING VULNERABILITIES ==========
            'tensorflow_load_unsafe': {
                'pattern': r'tf\.keras\.models\.load_model',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'TensorFlow model loading may execute code via Lambda layers',
                'recommendation': 'Set custom_objects=None or validate custom objects'
            },
            'onnx_load': {
                'pattern': r'onnx\.load(?:_model)?',
                'severity': 'MEDIUM',
                'cwe': 'CWE-502',
                'description': 'ONNX model loading should validate source',
                'recommendation': 'Verify model source and use onnx.checker.check_model()'
            },
            'transformers_from_pretrained': {
                'pattern': r'from_pretrained\([^)]*trust_remote_code\s*=\s*True',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Hugging Face transformers with trust_remote_code=True executes remote code',
                'recommendation': 'Only use trust_remote_code=True with verified models'
            },
            
            # ========== CODE INJECTION ==========
            'eval_usage': {
                'pattern': r'\beval\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-95',
                'description': 'eval() usage - code injection vulnerability',
                'recommendation': 'Use ast.literal_eval() or avoid eval entirely'
            },
            'exec_usage': {
                'pattern': r'\bexec\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-95',
                'description': 'exec() usage - code injection vulnerability',
                'recommendation': 'Refactor to avoid dynamic code execution'
            },
            
            # ========== COMMAND INJECTION ==========
            'subprocess_shell': {
                'pattern': r'subprocess\.\w+\([^)]*shell\s*=\s*True',
                'severity': 'CRITICAL',
                'cwe': 'CWE-78',
                'description': 'subprocess with shell=True allows command injection',
                'recommendation': 'Use shell=False and pass args as list'
            },
            'os_system': {
                'pattern': r'os\.system\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-78',
                'description': 'os.system() is vulnerable to command injection',
                'recommendation': 'Use subprocess.run() with shell=False'
            },
            'os_popen': {
                'pattern': r'os\.popen\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-78',
                'description': 'os.popen() is vulnerable to command injection',
                'recommendation': 'Use subprocess.Popen() with shell=False'
            },
            
            # ========== PATH TRAVERSAL ==========
            'path_join_unsafe': {
                'pattern': r'os\.path\.join\([^,)]*request\.|os\.path\.join\([^,)]*user_input',
                'severity': 'HIGH',
                'cwe': 'CWE-22',
                'description': 'Path joining with user input may allow directory traversal',
                'recommendation': 'Validate and sanitize paths, use os.path.abspath()'
            },
            'open_user_input': {
                'pattern': r'open\([^)]*(?:request\.|user_input|input\()',
                'severity': 'HIGH',
                'cwe': 'CWE-22',
                'description': 'Opening files with user input may allow unauthorized access',
                'recommendation': 'Validate file paths and restrict to safe directories'
            },
            
            # ========== SQL INJECTION ==========
            'sql_format_string': {
                'pattern': r'(?:SELECT|INSERT|UPDATE|DELETE).*(?:%s|\.format\(|\+.*(?:request\.|user_input))',
                'severity': 'HIGH',
                'cwe': 'CWE-89',
                'description': 'SQL query construction with string formatting',
                'recommendation': 'Use parameterized queries'
            },
            'sql_f_string': {
                'pattern': r'(?:SELECT|INSERT|UPDATE|DELETE).*f["\']',
                'severity': 'HIGH',
                'cwe': 'CWE-89',
                'description': 'SQL query using f-strings - potential injection',
                'recommendation': 'Use parameterized queries instead of f-strings'
            },
            
            # ========== HARDCODED SECRETS ==========
            'hardcoded_api_key': {
                'pattern': r'(?:api_key|apikey|api_secret|secret_key)\s*=\s*["\'][^"\']{10,}["\']',
                'severity': 'HIGH',
                'cwe': 'CWE-798',
                'description': 'Hardcoded API key or secret detected',
                'recommendation': 'Use environment variables or secure key management'
            },
            'hardcoded_password': {
                'pattern': r'(?:password|passwd|pwd)\s*=\s*["\'][^"\']+["\']',
                'severity': 'HIGH',
                'cwe': 'CWE-798',
                'description': 'Hardcoded password detected',
                'recommendation': 'Use environment variables or secure credential storage'
            },
            'aws_credentials': {
                'pattern': r'(?:aws_access_key_id|aws_secret_access_key)\s*=\s*["\'][^"\']+["\']',
                'severity': 'CRITICAL',
                'cwe': 'CWE-798',
                'description': 'AWS credentials hardcoded',
                'recommendation': 'Use AWS IAM roles or environment variables'
            },
            
            # ========== SSL/TLS ISSUES ==========
            'ssl_verify_false': {
                'pattern': r'verify\s*=\s*False|ssl\.CERT_NONE|check_hostname\s*=\s*False',
                'severity': 'MEDIUM',
                'cwe': 'CWE-295',
                'description': 'SSL/TLS certificate verification disabled',
                'recommendation': 'Enable certificate verification'
            },
            'requests_no_verify': {
                'pattern': r'requests\.\w+\([^)]*verify\s*=\s*False',
                'severity': 'MEDIUM',
                'cwe': 'CWE-295',
                'description': 'HTTPS request without certificate verification',
                'recommendation': 'Remove verify=False or set to True'
            },
            
            # ========== WEAK CRYPTOGRAPHY ==========
            'md5_usage': {
                'pattern': r'hashlib\.md5\(|MD5\.new\(',
                'severity': 'MEDIUM',
                'cwe': 'CWE-328',
                'description': 'MD5 is cryptographically broken',
                'recommendation': 'Use SHA-256 or stronger'
            },
            'sha1_usage': {
                'pattern': r'hashlib\.sha1\(|SHA1\.new\(',
                'severity': 'MEDIUM',
                'cwe': 'CWE-328',
                'description': 'SHA-1 is cryptographically weak',
                'recommendation': 'Use SHA-256 or stronger'
            },
            'weak_random': {
                'pattern': r'random\.(?:random|randint|choice)\(',
                'severity': 'LOW',
                'cwe': 'CWE-338',
                'description': 'Weak random number generator for security',
                'recommendation': 'Use secrets module for security-sensitive randomness'
            },
            'random_seed_fixed': {
                'pattern': r'random\.seed\(\s*\d+\s*\)|numpy\.random\.seed\(\s*\d+\s*\)',
                'severity': 'LOW',
                'cwe': 'CWE-338',
                'description': 'Fixed random seed in production code',
                'recommendation': 'Remove fixed seeds in production'
            },
            
            # ========== DEBUG/LOGGING ISSUES ==========
            'debugger_enabled': {
                'pattern': r'app\.run\([^)]*debug\s*=\s*True|DEBUG\s*=\s*True',
                'severity': 'MEDIUM',
                'cwe': 'CWE-489',
                'description': 'Debug mode enabled in production code',
                'recommendation': 'Disable debug mode in production'
            },
            'flask_debug': {
                'pattern': r'app\.debug\s*=\s*True|flask\.Flask\([^)]*debug\s*=\s*True',
                'severity': 'MEDIUM',
                'cwe': 'CWE-489',
                'description': 'Flask debug mode enabled',
                'recommendation': 'Set debug=False in production'
            },
            
            # ========== MISC VULNERABILITIES ==========
            'assert_used': {
                'pattern': r'assert\s+',
                'severity': 'LOW',
                'cwe': 'CWE-617',
                'description': 'Assert statements can be disabled with -O flag',
                'recommendation': 'Use explicit validation instead of assert'
            },
            'tempfile_mktemp': {
                'pattern': r'tempfile\.mktemp\(',
                'severity': 'MEDIUM',
                'cwe': 'CWE-377',
                'description': 'mktemp() is subject to race conditions',
                'recommendation': 'Use tempfile.mkstemp() or tempfile.TemporaryFile()'
            }
        }
    
    def _check_torch_load(self, match_text: str) -> bool:
        """Special check for torch.load - returns True if vulnerable"""
        return 'weights_only=True' not in match_text
    
    def scan_file(self, filepath: Path) -> List[Dict]:
        """Scan a single file for vulnerabilities"""
        self.scan_count += 1
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                lines = content.split('\n')
        except Exception as e:
            return []
        
        file_vulns = []
        
        for vuln_type, config in self.patterns.items():
            pattern = config['pattern']
            
            for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                # Special handling for patterns with custom checks
                if 'check' in config:
                    if not config['check'](match.group()):
                        continue
                
                line_num = content[:match.start()].count('\n') + 1
                
                # Skip if in comment
                if line_num <= len(lines):
                    line = lines[line_num - 1]
                    if line.strip().startswith('#'):
                        continue
                
                vuln = {
                    'file': str(filepath),
                    'line': line_num,
                    'type': vuln_type,
                    'severity': config['severity'],
                    'cwe': config.get('cwe', 'N/A'),
                    'description': config['description'],
                    'recommendation': config.get('recommendation', ''),
                    'code_snippet': lines[line_num-1].strip()[:100] if line_num <= len(lines) else ''
                }
                
                file_vulns.append(vuln)
        
        return file_vulns
    
    def scan_directory(self, directory: str, exclude_tests: bool = False, 
                       exclude_venv: bool = True, max_files: Optional[int] = None) -> List[Dict]:
        """Scan all Python files in directory"""
        path = Path(directory)
        
        if not path.exists():
            print(f"Error: Path {directory} does not exist")
            return []
        
        # Find all Python files
        py_files = list(path.rglob('*.py'))
        
        # Apply exclusions
        if exclude_tests:
            py_files = [f for f in py_files if '/test' not in str(f) and '/tests' not in str(f)]
        
        if exclude_venv:
            py_files = [f for f in py_files if '/venv/' not in str(f) and '/env/' not in str(f) 
                       and '/.env' not in str(f) and '/site-packages/' not in str(f)]
        
        # Limit files if specified
        if max_files:
            py_files = py_files[:max_files]
        
        print(f"\n[*] Scanning {len(py_files)} Python files...")
        print(f"[*] Exclude tests: {exclude_tests}")
        print(f"[*] Exclude venv: {exclude_venv}")
        
        for i, py_file in enumerate(py_files, 1):
            # Progress indicator
            if i % 100 == 0 or i == len(py_files):
                print(f"    Progress: {i}/{len(py_files)} files...")
            
            vulns = self.scan_file(py_file)
            self.vulnerabilities.extend(vulns)
        
        return self.vulnerabilities
    
    def generate_report(self, output_format: str = 'both', top_n: int = 10):
        """Generate vulnerability report"""
        if not self.vulnerabilities:
            print("\n‚úÖ No vulnerabilities found!")
            return
        
        # Group by severity
        severity_groups = {}
        severity_order = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
        
        for vuln in self.vulnerabilities:
            sev = vuln['severity']
            if sev not in severity_groups:
                severity_groups[sev] = []
            severity_groups[sev].append(vuln)
        
        # Console output
        print("\n" + "="*60)
        print("SPR{K}3 VULNERABILITY SCAN REPORT v4.4")
        print("="*60)
        print(f"\nüìä Summary:")
        print(f"   Files scanned: {self.scan_count}")
        print(f"   Total vulnerabilities: {len(self.vulnerabilities)}")
        
        for severity in severity_order:
            if severity in severity_groups:
                count = len(severity_groups[severity])
                emoji = {'CRITICAL': 'üö®', 'HIGH': '‚ö†Ô∏è', 'MEDIUM': '‚ö°', 'LOW': '‚ÑπÔ∏è'}.get(severity, '')
                print(f"   {emoji} {severity}: {count}")
        
        # Show top vulnerabilities by severity
        for severity in severity_order[:2]:  # Show only CRITICAL and HIGH
            if severity in severity_groups:
                vulns = severity_groups[severity]
                print(f"\n{{'üö®' if severity == 'CRITICAL' else '‚ö†Ô∏è'}} {severity} Vulnerabilities (top {min(top_n, len(vulns))}):")
                
                # Group by type for better readability
                by_type = {}
                for vuln in vulns:
                    if vuln['type'] not in by_type:
                        by_type[vuln['type']] = []
                    by_type[vuln['type']].append(vuln)
                
                shown = 0
                for vuln_type, type_vulns in sorted(by_type.items(), key=lambda x: -len(x[1])):
                    if shown >= top_n:
                        break
                    
                    print(f"\n   [{vuln_type}] - {len(type_vulns)} instances")
                    print(f"   {type_vulns[0]['description']}")
                    print(f"   Fix: {type_vulns[0]['recommendation']}")
                    
                    for vuln in type_vulns[:3]:  # Show first 3 examples
                        rel_path = vuln['file'].replace(os.path.expanduser('~'), '~')
                        print(f"      Example: {rel_path}:{vuln['line']}")
                    
                    shown += len(type_vulns[:3])
        
        # Save JSON report
        if output_format in ['json', 'both']:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"sprk3_scan_v44_{timestamp}.json"
            
            report_data = {
                'scan_metadata': {
                    'scanner_version': 'SPR{K}3 v4.4',
                    'scan_date': datetime.now().isoformat(),
                    'files_scanned': self.scan_count,
                    'total_vulnerabilities': len(self.vulnerabilities),
                    'patterns_checked': len(self.patterns)
                },
                'summary': {sev: len(vulns) for sev, vulns in severity_groups.items()},
                'vulnerability_types': {
                    vuln_type: len([v for v in self.vulnerabilities if v['type'] == vuln_type])
                    for vuln_type in set(v['type'] for v in self.vulnerabilities)
                },
                'vulnerabilities': self.vulnerabilities[:1000]  # Limit to prevent huge files
            }
            
            with open(report_file, 'w') as f:
                json.dump(report_data, f, indent=2)
            
            print(f"\nüìÅ Detailed report saved to: {report_file}")
        
        # Statistics
        print(f"\nüìà Vulnerability Types Found:")
        type_counts = {}
        for vuln in self.vulnerabilities:
            type_counts[vuln['type']] = type_counts.get(vuln['type'], 0) + 1
        
        for vuln_type, count in sorted(type_counts.items(), key=lambda x: -x[1])[:10]:
            print(f"   {vuln_type}: {count}")

def main():
    """Main entry point"""
    import argparse
    
    print("\n" + "="*60)
    print("  SPR{K}3 Vulnerability Scanner v4.4")
    print("  Comprehensive ML/AI Security Analysis")
    print("  42 vulnerability patterns | 7 categories")
    print("="*60)
    
    parser = argparse.ArgumentParser(description='Scan ML/AI code for security vulnerabilities')
    parser.add_argument('path', help='Directory or file to scan')
    parser.add_argument('--exclude-tests', action='store_true', help='Exclude test directories')
    parser.add_argument('--exclude-venv', action='store_true', default=True, help='Exclude virtual environments')
    parser.add_argument('--include-venv', action='store_true', help='Include virtual environments')
    parser.add_argument('--format', choices=['console', 'json', 'both'], default='both',
                       help='Output format (default: both)')
    parser.add_argument('--top', type=int, default=10, help='Number of top vulnerabilities to show')
    parser.add_argument('--max-files', type=int, help='Maximum number of files to scan')
    
    args = parser.parse_args()
    
    # Handle venv exclusion
    exclude_venv = not args.include_venv if args.include_venv else args.exclude_venv
    
    scanner = SPRk3VulnerabilityScanner()
    
    if os.path.isfile(args.path):
        vulns = scanner.scan_file(Path(args.path))
        scanner.vulnerabilities = vulns
    else:
        scanner.scan_directory(
            args.path, 
            exclude_tests=args.exclude_tests,
            exclude_venv=exclude_venv,
            max_files=args.max_files
        )
    
    scanner.generate_report(output_format=args.format, top_n=args.top)
    
    # Exit code based on findings
    if scanner.vulnerabilities:
        severity_list = [v['severity'] for v in scanner.vulnerabilities]
        if 'CRITICAL' in severity_list:
            sys.exit(2)  # Critical vulnerabilities found
        elif 'HIGH' in severity_list:
            sys.exit(1)  # High vulnerabilities found
    
    sys.exit(0)  # Clean

if __name__ == "__main__":
    main()
