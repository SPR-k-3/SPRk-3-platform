#!/usr/bin/env python3
"""
SPR{K}3 Vulnerability Scanner v4.4.2
Production-ready ML/AI security scanner with SARIF output and baseline support
Enhanced with multi-line detection, precise grading, and CI integration
Purpose: Defensive security tool to help developers find and fix vulnerabilities
License: Responsible use only - for security research and improvement
"""

import os
import sys
import re
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set

class SPRk3VulnerabilityScanner:
    """
    Production-grade security vulnerability detection for ML/AI codebases
    v4.4.2: SARIF output, baseline suppressions, golden tests, improved patterns
    """
    
    def __init__(self):
        self.vulnerabilities = []
        self.scan_count = 0
        self.flags = re.IGNORECASE | re.MULTILINE | re.DOTALL
        self.compiled = {}
        
        # 25 vulnerability patterns across 10 categories
        self.patterns = {
            # ========== DESERIALIZATION (7 patterns) ==========
            'torch_unsafe': {
                'pattern': r'torch\.load\s*\((?P<args>.*?)\)',
                'check': self._check_torch_load,
                'severity': 'CRITICAL',
                'cwe': 'CWE-502',
                'description': 'Unsafe torch.load() - potential arbitrary code execution',
                'recommendation': 'Add weights_only=True parameter'
            },
            'pickle_unsafe': {
                'pattern': r'pickle\.(load|loads)\s*\(',
                'severity': 'CRITICAL',
                'cwe': 'CWE-502',
                'description': 'Unsafe pickle deserialization - arbitrary code execution risk',
                'recommendation': 'Use safer serialization formats (JSON, MessagePack)'
            },
            'dill_load': {
                'pattern': r'dill\.(load|loads)\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'dill.load() can execute arbitrary code',
                'recommendation': 'Use safer serialization methods'
            },
            'joblib_load': {
                'pattern': r'joblib\.load\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'joblib.load relies on pickle; arbitrary code if untrusted',
                'recommendation': 'Use trusted sources only, consider ONNX export'
            },
            'numpy_load': {
                'pattern': r'numpy\.load\s*\([^)]*allow_pickle\s*=\s*True',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'NumPy load with allow_pickle=True can execute code',
                'recommendation': 'Set allow_pickle=False'
            },
            'pandas_read_pickle': {
                'pattern': r'pd\.read_pickle|pandas\.read_pickle',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Pandas read_pickle can execute arbitrary code',
                'recommendation': 'Use safer formats like parquet or feather'
            },
            'yaml_unsafe': {
                'pattern': r'yaml\.load\s*\((?P<yargs>.*?)\)',
                'check': self._check_yaml_load,
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Potentially unsafe YAML loading',
                'recommendation': 'Prefer yaml.safe_load(); avoid UnsafeLoader'
            },
            
            # ========== MODEL LOADING (3 patterns) ==========
            'tensorflow_load_unsafe': {
                'pattern': r'tf\.keras\.models\.load_model',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'TensorFlow model loading may execute code via Lambda layers',
                'recommendation': 'Set custom_objects=None or validate custom objects'
            },
            'onnx_load': {
                'pattern': r'onnx\.load(?:_model)?',
                'severity': 'MEDIUM',
                'cwe': 'CWE-502',
                'description': 'ONNX model loading should validate source',
                'recommendation': 'Verify model source and use onnx.checker.check_model()'
            },
            'transformers_from_pretrained': {
                'pattern': r'from_pretrained\s*\((?:(?!\)).)*trust_remote_code\s*=\s*True',
                'severity': 'HIGH',
                'cwe': 'CWE-502',
                'description': 'Hugging Face transformers with trust_remote_code=True executes remote code',
                'recommendation': 'Only use trust_remote_code=True with verified models'
            },
            
            # ========== CODE INJECTION (2 patterns) ==========
            'eval_usage': {
                'pattern': r'\beval\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-95',
                'description': 'eval() usage - code injection vulnerability',
                'recommendation': 'Use ast.literal_eval() or avoid eval entirely'
            },
            'exec_usage': {
                'pattern': r'\bexec\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-95',
                'description': 'exec() usage - code injection vulnerability',
                'recommendation': 'Refactor to avoid dynamic code execution'
            },
            
            # ========== COMMAND INJECTION (3 patterns) ==========
            'subprocess_shell': {
                'pattern': r'subprocess\.\w+\s*\([^)]*shell\s*=\s*True',
                'severity': 'CRITICAL',
                'cwe': 'CWE-78',
                'description': 'subprocess with shell=True allows command injection',
                'recommendation': 'Use shell=False and pass args as list'
            },
            'os_system': {
                'pattern': r'os\.system\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-78',
                'description': 'os.system() is vulnerable to command injection',
                'recommendation': 'Use subprocess.run() with shell=False'
            },
            'os_popen': {
                'pattern': r'os\.popen\s*\(',
                'severity': 'HIGH',
                'cwe': 'CWE-78',
                'description': 'os.popen() is vulnerable to command injection',
                'recommendation': 'Use subprocess.Popen() with shell=False'
            },
            
            # ========== SQL INJECTION (2 patterns) ==========
            'sql_format_string': {
                'pattern': r'(?is)^\s*(SELECT|INSERT|UPDATE|DELETE).*?(%s|\.format\(|\+\s*(?:request\.|user_input))',
                'severity': 'HIGH',
                'cwe': 'CWE-89',
                'description': 'SQL query construction with string formatting',
                'recommendation': 'Use parameterized queries'
            },
            'sql_f_string': {
                'pattern': r'(?is)^\s*f["\'].*?\b(SELECT|INSERT|UPDATE|DELETE)\b',
                'severity': 'HIGH',
                'cwe': 'CWE-89',
                'description': 'SQL query using f-strings - potential injection',
                'recommendation': 'Use parameterized queries instead of f-strings'
            },
            
            # ========== SECRETS (2 patterns) ==========
            'hardcoded_api_key': {
                'pattern': r'(?:api_key|apikey|api_secret|secret_key)\s*=\s*["\'][^"\']{20,}["\']',
                'severity': 'HIGH',
                'cwe': 'CWE-798',
                'description': 'Hardcoded API key or secret detected',
                'recommendation': 'Use environment variables or secure key management'
            },
            'aws_credentials': {
                'pattern': r'(?:AKIA[0-9A-Z]{16}|ASIA[0-9A-Z]{16}|aws_secret_access_key\s*=\s*["\'][A-Za-z0-9/+=]{35,}["\'])',
                'severity': 'CRITICAL',
                'cwe': 'CWE-798',
                'description': 'AWS credentials hardcoded',
                'recommendation': 'Use AWS IAM roles or environment variables'
            },
            
            # ========== SSL/TLS (1 pattern) ==========
            'ssl_verify_false': {
                'pattern': r'verify\s*=\s*False|ssl\.CERT_NONE|check_hostname\s*=\s*False',
                'severity': 'MEDIUM',
                'cwe': 'CWE-295',
                'description': 'SSL/TLS certificate verification disabled',
                'recommendation': 'Enable certificate verification'
            },
            
            # ========== CRYPTO (1 pattern) ==========
            'md5_usage': {
                'pattern': r'hashlib\.md5\s*\(|MD5\.new\s*\(',
                'severity': 'MEDIUM',
                'cwe': 'CWE-328',
                'description': 'MD5 is cryptographically broken',
                'recommendation': 'Use SHA-256 or stronger'
            },
            
            # ========== DEBUG (1 pattern) ==========
            'debugger_enabled': {
                'pattern': r'app\.run\s*\([^)]*debug\s*=\s*True|DEBUG\s*=\s*True',
                'severity': 'MEDIUM',
                'cwe': 'CWE-489',
                'description': 'Debug mode enabled',
                'recommendation': 'Disable debug mode in production'
            },
            
            # ========== MISC (2 patterns) ==========
            'assert_used': {
                'pattern': r'\bassert\s+',
                'severity': 'LOW',
                'cwe': 'CWE-617',
                'description': 'Assert statements can be disabled with -O flag',
                'recommendation': 'Use explicit validation instead of assert'
            },
            'weak_random': {
                'pattern': r'random\.(?:random|randint|choice)\s*\(',
                'severity': 'LOW',
                'cwe': 'CWE-338',
                'description': 'Weak random number generator',
                'recommendation': 'Use secrets module for security-sensitive randomness'
            }
        }
    
    def _check_torch_load(self, match_text: str) -> Optional[str]:
        """Check torch.load safety - returns severity or None if safe"""
        args = match_text
        if 'weights_only' not in args:
            return 'CRITICAL'
        if re.search(r'weights_only\s*=\s*False\b', args, re.IGNORECASE):
            return 'CRITICAL'
        if re.search(r'weights_only\s*=\s*True\b', args, re.IGNORECASE):
            return None  # Safe
        return 'HIGH'  # Variable/expression
    
    def _check_yaml_load(self, match_text: str) -> Optional[str]:
        """Check YAML loading safety"""
        if re.search(r'Loader\s*=\s*yaml\.UnsafeLoader', match_text):
            return 'HIGH'
        if re.search(r'Loader\s*=\s*yaml\.SafeLoader', match_text):
            return None  # Safe
        if re.search(r'Loader\s*=\s*yaml\.FullLoader', match_text):
            return 'MEDIUM'
        return 'HIGH'  # No Loader specified
    
    def _load_baseline(self, path: Optional[str]) -> Set:
        """Load baseline suppressions"""
        if not path:
            return set()
        try:
            with open(path, 'r') as f:
                data = json.load(f)
            return {(d["file"], int(d["line"]), d["type"]) for d in data}
        except Exception:
            return set()
    
    def _to_sarif(self) -> dict:
        """Generate SARIF v2.1.0 output for GitHub Code Scanning"""
        level_map = {"CRITICAL": "error", "HIGH": "error", "MEDIUM": "warning", "LOW": "note"}
        rules = {}
        results = []
        
        for v in self.vulnerabilities:
            rule_id = v["type"]
            if rule_id not in rules:
                rules[rule_id] = {
                    "id": rule_id,
                    "shortDescription": {"text": v["description"][:120]},
                    "fullDescription": {"text": v["description"]},
                    "help": {"text": v.get("recommendation", "")},
                    "properties": {"cwe": v.get("cwe", "N/A")}
                }
            
            results.append({
                "ruleId": rule_id,
                "level": level_map.get(v["severity"], "warning"),
                "message": {"text": f'{v["description"]} | Fix: {v.get("recommendation", "")}'},
                "locations": [{
                    "physicalLocation": {
                        "artifactLocation": {"uri": v["file"]},
                        "region": {"startLine": v["line"]}
                    }
                }]
            })
        
        return {
            "version": "2.1.0",
            "$schema": "https://json.schemastore.org/sarif-2.1.0.json",
            "runs": [{
                "tool": {
                    "driver": {
                        "name": "SPR{K}3 v4.4.2",
                        "informationUri": "https://github.com/SPR-k-3/SPRk-3-platform",
                        "rules": list(rules.values())
                    }
                },
                "results": results
            }]
        }
    
    def scan_file(self, filepath: Path) -> List[Dict]:
        """Scan a single file for vulnerabilities"""
        self.scan_count += 1
        
        # Skip huge files
        try:
            if filepath.stat().st_size > 1_000_000:  # 1MB
                return []
        except:
            pass
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                lines = content.split('\n')
        except Exception:
            return []
        
        # Compile patterns on first use
        if not self.compiled:
            for k, cfg in self.patterns.items():
                self.compiled[k] = re.compile(cfg['pattern'], self.flags)
        
        file_vulns = []
        
        for vuln_type, config in self.patterns.items():
            cre = self.compiled[vuln_type]
            
            for match in cre.finditer(content):
                sev_override = None
                if 'check' in config:
                    sev_override = config['check'](match.group())
                    if sev_override is False or sev_override is None:
                        continue  # Skip safe patterns
                
                line_num = content[:match.start()].count('\n') + 1
                
                # Skip if in comment
                if line_num <= len(lines):
                    line = lines[line_num - 1]
                    if line.strip().startswith('#'):
                        continue
                
                vuln = {
                    'file': str(filepath),
                    'line': line_num,
                    'type': vuln_type,
                    'severity': sev_override if isinstance(sev_override, str) else config['severity'],
                    'cwe': config.get('cwe', 'N/A'),
                    'description': config['description'],
                    'recommendation': config.get('recommendation', ''),
                    'code_snippet': lines[line_num-1].strip()[:100] if line_num <= len(lines) else ''
                }
                
                file_vulns.append(vuln)
        
        return file_vulns
    
    def scan_directory(self, directory: str, exclude_tests: bool = False,
                       exclude_venv: bool = True, max_files: Optional[int] = None) -> List[Dict]:
        """Scan all Python files in directory"""
        path = Path(directory)
        
        if not path.exists():
            print(f"Error: Path {directory} does not exist")
            return []
        
        py_files = list(path.rglob('*.py'))
        
        if exclude_tests:
            py_files = [f for f in py_files if '/test' not in str(f) and '/tests' not in str(f)]
        
        if exclude_venv:
            py_files = [f for f in py_files if '/venv/' not in str(f) and '/env/' not in str(f)
                       and '/.env' not in str(f) and '/site-packages/' not in str(f)]
        
        if max_files:
            py_files = py_files[:max_files]
        
        print(f"\n[*] Scanning {len(py_files)} Python files...")
        print(f"[*] Exclude tests: {exclude_tests}")
        print(f"[*] Exclude venv: {exclude_venv}")
        
        for i, py_file in enumerate(py_files, 1):
            if i % 100 == 0 or i == len(py_files):
                print(f"    Progress: {i}/{len(py_files)} files...")
            
            vulns = self.scan_file(py_file)
            self.vulnerabilities.extend(vulns)
        
        return self.vulnerabilities
    
    def generate_report(self, output_format: str = 'both', top_n: int = 10):
        """Generate vulnerability report"""
        if not self.vulnerabilities:
            print("\n‚úÖ No vulnerabilities found!")
            return
        
        severity_groups = {}
        severity_order = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
        
        for vuln in self.vulnerabilities:
            sev = vuln['severity']
            if sev not in severity_groups:
                severity_groups[sev] = []
            severity_groups[sev].append(vuln)
        
        print("\n" + "="*60)
        print("SPR{K}3 VULNERABILITY SCAN REPORT v4.4.2")
        print("="*60)
        print(f"\nüìä Summary:")
        print(f"   Files scanned: {self.scan_count}")
        print(f"   Total vulnerabilities: {len(self.vulnerabilities)}")
        
        for severity in severity_order:
            if severity in severity_groups:
                count = len(severity_groups[severity])
                emoji = {'CRITICAL': 'üö®', 'HIGH': '‚ö†Ô∏è', 'MEDIUM': '‚ö°', 'LOW': '‚ÑπÔ∏è'}.get(severity, '')
                print(f"   {emoji} {severity}: {count}")
        
        for severity in severity_order[:2]:
            if severity in severity_groups:
                vulns = severity_groups[severity]
                emoji = 'üö®' if severity == 'CRITICAL' else '‚ö†Ô∏è'
                print(f"\n{emoji} {severity} Vulnerabilities (top {min(top_n, len(vulns))}):")
                
                by_type = {}
                for vuln in vulns:
                    if vuln['type'] not in by_type:
                        by_type[vuln['type']] = []
                    by_type[vuln['type']].append(vuln)
                
                shown = 0
                for vuln_type, type_vulns in sorted(by_type.items(), key=lambda x: -len(x[1])):
                    if shown >= top_n:
                        break
                    
                    print(f"\n   [{vuln_type}] - {len(type_vulns)} instances")
                    print(f"   {type_vulns[0]['description']}")
                    print(f"   Fix: {type_vulns[0]['recommendation']}")
                    
                    for vuln in type_vulns[:3]:
                        rel_path = vuln['file'].replace(os.path.expanduser('~'), '~')
                        print(f"      Example: {rel_path}:{vuln['line']}")
                    
                    shown += len(type_vulns[:3])
        
        if output_format in ['json', 'both']:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"sprk3_scan_v442_{timestamp}.json"
            
            report_data = {
                'scan_metadata': {
                    'scanner_version': 'SPR{K}3 v4.4.2',
                    'scan_date': datetime.now().isoformat(),
                    'files_scanned': self.scan_count,
                    'total_vulnerabilities': len(self.vulnerabilities),
                    'patterns_checked': len(self.patterns)
                },
                'summary': {sev: len(vulns) for sev, vulns in severity_groups.items()},
                'vulnerabilities': self.vulnerabilities[:1000]
            }
            
            with open(report_file, 'w') as f:
                json.dump(report_data, f, indent=2)
            
            print(f"\nüìÅ JSON report: {report_file}")

def main():
    import argparse
    
    print("\n" + "="*60)
    print("  SPR{K}3 Vulnerability Scanner v4.4.2")
    print("  Production ML/AI Security Analysis")
    print("  25 patterns | 10 categories | SARIF + Baseline support")
    print("="*60)
    
    parser = argparse.ArgumentParser(description='Scan ML/AI code for security vulnerabilities')
    parser.add_argument('path', help='Directory or file to scan')
    parser.add_argument('--exclude-tests', action='store_true', help='Exclude test directories')
    parser.add_argument('--exclude-venv', action='store_true', default=True,
                       help='Exclude virtual environments (default)')
    parser.add_argument('--include-venv', dest='exclude_venv', action='store_false',
                       help='Include virtual environments')
    parser.add_argument('--format', choices=['console', 'json', 'both'], default='both',
                       help='Output format (default: both)')
    parser.add_argument('--top', type=int, default=10, help='Number of top vulnerabilities to show')
    parser.add_argument('--max-files', type=int, help='Maximum number of files to scan')
    parser.add_argument('--fail-on', choices=['CRITICAL','HIGH','MEDIUM','LOW','NONE'], default='HIGH',
                       help='Exit non-zero if findings at or above this severity (default: HIGH)')
    parser.add_argument('--sarif', action='store_true', help='Also write SARIF output (sprk3_v442.sarif)')
    parser.add_argument('--baseline', help='JSON file of accepted/suppressed findings')
    parser.add_argument('--write-baseline', action='store_true',
                       help='Write current findings to baseline.json and exit')
    
    args = parser.parse_args()
    
    scanner = SPRk3VulnerabilityScanner()
    
    if os.path.isfile(args.path):
        vulns = scanner.scan_file(Path(args.path))
        scanner.vulnerabilities = vulns
    else:
        scanner.scan_directory(
            args.path,
            exclude_tests=args.exclude_tests,
            exclude_venv=args.exclude_venv,
            max_files=args.max_files
        )
    
    # Apply baseline suppressions
    baseline = scanner._load_baseline(args.baseline)
    if baseline:
        original_count = len(scanner.vulnerabilities)
        scanner.vulnerabilities = [v for v in scanner.vulnerabilities
                                   if (v["file"], v["line"], v["type"]) not in baseline]
        print(f"\n[*] Baseline: Suppressed {original_count - len(scanner.vulnerabilities)} known findings")
    
    # Write baseline if requested
    if args.write_baseline:
        baseline_data = [{"file": v["file"], "line": v["line"], "type": v["type"]}
                        for v in scanner.vulnerabilities]
        with open('baseline.json', 'w') as f:
            json.dump(baseline_data, f, indent=2)
        print("\nüìÑ Wrote baseline.json with current findings")
        sys.exit(0)
    
    scanner.generate_report(output_format=args.format, top_n=args.top)
    
    # Generate SARIF if requested
    if args.sarif and scanner.vulnerabilities:
        sarif = scanner._to_sarif()
        with open("sprk3_v442.sarif", "w") as f:
            json.dump(sarif, f, indent=2)
        print("üóÇ  SARIF written: sprk3_v442.sarif")
    
    # Exit code based on findings and threshold
    if scanner.vulnerabilities and args.fail_on != 'NONE':
        order = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
        threshold_idx = order.index(args.fail_on)
        
        for vuln in scanner.vulnerabilities:
            if order.index(vuln['severity']) <= threshold_idx:
                sys.exit(1)
    
    sys.exit(0)

if __name__ == "__main__":
    main()
