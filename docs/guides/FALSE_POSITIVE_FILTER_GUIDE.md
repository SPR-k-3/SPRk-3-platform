# SPR{K}3 False Positive Filter Guide

## What This Does

Your scanner found **78 vulnerabilities**, but many are **false positives** that will be rejected if submitted to bug bounty programs. This filter removes ~50-80% noise.

---

## Common False Positives (With Examples)

### âŒ **FastAPI (12 findings) â€” All Likely False Positives**

```python
heroes = session.exec(select(Hero).offset(offset).limit(limit)).all()
```

**Problem**: This is **SQLModel's ORM method**, NOT Python's dangerous `exec()`
- âœ… Safe database query
- âœ… Used everywhere in SQLAlchemy
- âŒ Will be rejected immediately if submitted

**Filter Action**: â›” **Remove** (0% confidence)

---

### âŒ **PyTorch Lightning (54 findings) â€” Likely False/Intentional**

```python
state_dict = torch.load(single_ckpt_path, weights_only=False)
```

**Problem**: Developers **explicitly chose** `weights_only=False`
- They're **aware** this parameter exists
- They **intentionally** set it unsafe for their use case
- âŒ Not a vulnerability â€” it's by design

**Filter Action**: â›” **Remove** (0% confidence)

---

### âš ï¸ **Gradio (8 findings) â€” Mixed (Need Manual Check)**

```python
exec(no_reload_source_code, module.__dict__)
```

**Problem**: Could be legitimate, but need context
- Is input user-controlled?
- Is it in test code?
- Is there a guard checking input?

**Filter Action**: ğŸ” **Keep but reduce confidence** (0.3-0.7)

---

### ğŸŸ¢ **Genuinely Dangerous (Real Submissions)**

```python
def load_user_model(user_input_path):
    model = torch.load(user_input_path)  # User controls path!
    return model
```

**Why It's Real**:
- âœ… User controls the path
- âœ… No validation or guards
- âœ… Arbitrary code execution possible
- âœ… Production code (not test)

**Filter Action**: âœ… **Keep** (0.8-0.95 confidence)

---

## How The Filter Works

### **Stage 1: ORM Detection**
Checks if `exec()` is actually:
- `session.exec()` (SQLAlchemy ORM) â†’ âŒ Remove
- `db.session.query()` (Django ORM) â†’ âŒ Remove
- `await Model.filter()` (Tortoise ORM) â†’ âŒ Remove
- Python's `exec()` â†’ âœ… Keep

### **Stage 2: Test/Example Code Detection**
Filters out:
- Files in `tests/`, `examples/`, `docs/` â†’ âŒ Remove
- Code in docstrings (`>>>`, `"""..."""`) â†’ âŒ Remove
- Test functions (`def test_*`) â†’ âŒ Remove

### **Stage 3: Intentional Pattern Detection**
Identifies:
- `torch.load(..., weights_only=False)` + comment â†’ âŒ Remove
- `pickle.load` marked as "trusted source" â†’ âŒ Remove
- `yaml.load` with `SafeLoader` â†’ âœ… Keep (not unsafe)

### **Stage 4: Safety Exclusion Check**
Looks for patterns that make it safe:
- `weights_only=True` â†’ âŒ Remove
- `ast.literal_eval` (safe eval) â†’ âŒ Remove
- Try/except wrapping â†’ ğŸŸ¡ Reduce confidence

### **Stage 5: Context Analysis**
Adjusts confidence based on:
- Is input user-controlled?
- Is there input validation?
- Is it in production vs. test?
- Are there guards/checks?

---

## Expected Results After Filtering

| Company | Before | After | Type | Keep % |
|---------|--------|-------|------|--------|
| **FastAPI** | 12 | 0-2 | ORM false positives | 0-17% |
| **PyTorch Lightning** | 54 | 5-10 | Intentional unsafe | 9-19% |
| **Gradio** | 8 | 3-5 | Mixed, need review | 37-62% |
| **Streamlit** | 4 | 1-3 | Mixed exec() calls | 25-75% |
| **TOTAL** | 78 | **10-20** | After filtering | **13-26%** |

**Realistic Bounty After Filtering**: $50,000-$150,000 (not $624K)

---

## Real-World Examples of What Gets Filtered

### âœ… KEEPS These (Real Vulnerabilities)

```python
# Gradio - arbitrary code execution in production
exec(code)

# Streamlit - arbitrary code execution
exec(code, module.__dict__)

# PyTorch without explicit safety parameter
model = torch.load(checkpoint_path)  # No weights_only param at all
```

### âŒ REMOVES These (False Positives)

```python
# FastAPI - ORM method, not exec()
heroes = session.exec(select(Hero))

# PyTorch - intentionally unsafe by design
state_dict = torch.load(path, weights_only=False)

# Documentation - not production code
>>> model = torch.load("example.pth")
```

---

## How to Use

### **Step 1: Copy Files**
```bash
# Files already in your directory:
# - sprk3_false_positive_filter.py
# - sprk3_processor.py
# - sprk3_vulnerabilities.json (from scanner)
```

### **Step 2: Run Filter**
```bash
python3 sprk3_processor.py sprk3_vulnerabilities.json
```

### **Step 3: Review Output**
Three new files created:
```
sprk3_vulnerabilities_filtered.json    # Machine-readable cleaned data
sprk3_bounty_report_filtered.md        # Human-readable report
```

### **Step 4: Check Results**
The report shows:
- âœ… What was filtered and why
- âœ… Confidence scores adjusted
- âœ… Real bounty potential
- âœ… Which company has most real findings

---

## What The Filter Actually Removes

Based on your 78 findings:

### **FastAPI (12) â†’ 0 Real**
- âœ… All 12 are `session.exec()` (ORM method)
- âŒ Confidence reduced to 0%
- **Action**: Don't submit these

### **PyTorch Lightning (54) â†’ 5-10 Real**
- âœ… Most have explicit `weights_only=False`
- âš ï¸ A few might be real (missing parameter entirely)
- ğŸ“Š Keep 9-19% after filtering

### **Gradio (8) â†’ 3-5 Real**
- âœ… Mix of real exec() and safe patterns
- ğŸ” Need manual verification
- ğŸ“Š Keep 37-62% after filtering

### **Streamlit (4) â†’ 1-3 Real**
- âœ… Some real exec() calls
- âš ï¸ Some might be in test code
- ğŸ“Š Keep 25-75% after filtering

---

## CRITICAL: Before Submitting

Even after filtering, **manually verify findings**:

1. **Check if input is user-controlled**
   ```python
   # Dangerous - user controls path
   model = torch.load(user_provided_path)
   
   # Safe - hardcoded path
   model = torch.load("/app/models/default.pth")
   ```

2. **Check for guards/validation**
   ```python
   # Dangerous - no validation
   exec(code)
   
   # Safer - some validation (but still risky)
   if is_safe_code(code):
       exec(code)
   ```

3. **Check production vs. test context**
   ```python
   # Test code - don't submit
   tests/test_loading.py:
       model = torch.load(test_file)
   
   # Production code - submit
   src/models/loader.py:
       model = torch.load(user_path)
   ```

4. **Check for documented reasons**
   ```python
   # Has documentation - intentional
   # PyTorch requires weights_only=False for legacy models
   state_dict = torch.load(path, weights_only=False)
   
   # No documentation - real vulnerability
   exec(user_code)
   ```

---

## Success Metrics

After filtering, you should have:
- âœ… 10-20 real vulnerabilities (not 78)
- âœ… 50-80% false positives removed
- âœ… Confidence scores adjusted appropriately
- âœ… Clear reason for each removal
- âœ… Realistic bounty estimate ($50K-$150K)

---

## Next Steps After Filtering

1. **Review the filtered report** - See which ones are real
2. **Manually verify top 3-5 findings** - Create POC if possible
3. **Submit to official bug bounty programs** - Not Twitter/Reddit!
4. **Track responses** - Companies typically respond in 5-10 days
5. **Update scanner** - Add feedback from what was accepted/rejected

---

## Questions?

- ğŸ’¡ **Why was mine removed?** Check the "filter_reason" field in JSON
- ğŸ¯ **Is this one real?** Look at: Is input user-controlled + No guards + Production code
- ğŸ“Š **What's the real bounty?** Look at filtered JSON for realistic total
- ğŸš€ **Ready to submit?** Start with Databricks/AWS (most generous programs)

